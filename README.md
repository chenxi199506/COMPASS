Building Reliable Evidence with AI: An Orchestrated Multi-Agent Framework and Methodological Audit of 1.2 Million Randomized Controlled Trials

ğŸ” Overview
This project introduces a scalable AI framework for reliable evidence synthesis by orchestrating multiple large language models to audit 1.2 million RCTsâ€”the largest methodological evaluation of its kind. We address the critical challenge of LLM hallucination in scientific literature processing and propose a systematic solution that combines multi-agent consensus, structured quality control, and an interactive analysis platform.

The system comprises three core innovations:

ACCORD â€“ A multi-agent orchestration framework that mitigates LLM hallucinations through task planning, weighted voting, and expert evaluation, achieving higher accuracy than any single model.

RCT Knowledge Base â€“ A structured repository of 490,430 high-quality RCTs extracted from 1.2 million publications, enabling large-scale methodological audits.

COMPASS â€“ The first online platform integrating multi-agent extraction with automated meta-analysis workflows, supporting real-time evidence synthesis and methodological exploration.

ğŸš€ Explore the COMPASS Platform
ğŸŒ ğŸ‘‰ Visit the LIVE COMPASS Platform
Access COMPASS, our interactive platform for agent-driven evidence synthesis and methodological auditing of RCTs. The platform enables:

Automated literature screening and data extraction via the ACCORD multi-agent engine

Interactive meta-analysis with real-time forest plots and bias diagnostics

Large-scale methodological dashboards visualizing statistical power, p-hacking patterns, and temporal trends across 490,000+ RCTs

https://github.com/chenxi199506/ASAP/blob/master/COMPASS_preview.png

(If the preview does not load on GitHub, please click the link above to open the live version.)

ğŸ§© System Architecture
ACCORD Multi-Agent Framework
The framework operates through four coordinated stages:

Task Planning â€“ Decomposes extraction tasks into structured subtasks

Targeted Distribution â€“ Assigns subtasks to LLMs based on benchmarked strengths

Weighted Ensemble Voting â€“ Aggregates outputs using confidence-weighted consensus

Expert Evaluation â€“ Implements closed-loop validation and calibration

COMPASS Platform Integration
Frontend: Interactive Shiny application for visualization and user-directed analysis

Backend: ACCORD agent orchestration engine and structured RCT database

Workflow Support: Fully automated, semi-automated, and manual meta-analysis pipelines

ğŸ“Š Key Findings from 1.2 Million RCTs
Statistical Power: Only 60.4% of RCTs were adequately powered (â‰¥80% power), though this proportion has improved over time.

P-Hacking Patterns: Prevalence follows a U-shaped distributionâ€”highest in mid-tier journals and among both highly cited and uncited papers.

Model Performance: Benchmarking of 26 LLMs showed significant task-dependent variation, with no single model dominating across all extraction tasks.

System Accuracy: The ACCORD framework achieved higher consensus accuracy than any individual LLM, effectively mitigating hallucination through orchestrated verification.

ğŸ’¡ Implications & Future Directions
Reliable AI for Evidence Synthesis: Demonstrates that intelligent system designâ€”beyond raw model capabilityâ€”is critical for deploying trustworthy AI in biomedical research.

Continuous Methodological Monitoring: The structured RCT database enables ongoing audit of research quality, transparency, and statistical rigor across disciplines.

Scalable Synthesis Infrastructure: COMPASS provides a foundational platform for next-generation, high-throughput evidence synthesis that keeps pace with literature growth.

Open Science & Community Use: The platform and framework are designed for broad adoption, supporting reproducible meta-analysis and methodological research.

ğŸ“ Repository & Data Availability
COMPASS Platform: https://chatgptmodel.shinyapps.io/COMPASS/

ACCORD Framework Code: Available on GitHub (link to be added upon publication)

Structured RCT Database: Subset available for methodological research upon request

Benchmark Dataset: 1,049 expert-annotated publications for LLM evaluation

ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç®€ä»‹
ğŸ” æ¦‚è¿°
æœ¬ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„AIè¯æ®åˆæˆæ¡†æ¶ï¼Œé€šè¿‡åè°ƒå¤šä¸ªå¤§è¯­è¨€æ¨¡å‹å¯¹120ä¸‡é¡¹éšæœºå¯¹ç…§è¯•éªŒï¼ˆRCTï¼‰è¿›è¡Œäº†æ–¹æ³•å­¦å®¡è®¡â€”â€”è¿™æ˜¯è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„åŒç±»è¯„ä¼°ã€‚æˆ‘ä»¬è§£å†³äº†LLMåœ¨ç§‘å­¦æ–‡çŒ®å¤„ç†ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€å¥—ç»“åˆå¤šæ™ºèƒ½ä½“å…±è¯†ã€ç»“æ„åŒ–è´¨é‡æ§åˆ¶ä¸äº¤äº’å¼åˆ†æå¹³å°çš„ç³»ç»Ÿè§£å†³æ–¹æ¡ˆã€‚

ç³»ç»ŸåŒ…å«ä¸‰å¤§æ ¸å¿ƒåˆ›æ–°ï¼š

ACCORD â€“ ä¸€ä¸ªå¤šæ™ºèƒ½ä½“åè°ƒæ¡†æ¶ï¼Œé€šè¿‡ä»»åŠ¡è§„åˆ’ã€åŠ æƒæŠ•ç¥¨ä¸ä¸“å®¶è¯„ä¼°æ¥ç¼“è§£LLMå¹»è§‰ï¼Œå‡†ç¡®ç‡è¶…è¶Šä»»ä½•å•ä¸€æ¨¡å‹ã€‚

RCTçŸ¥è¯†åº“ â€“ ä»120ä¸‡ç¯‡æ–‡çŒ®ä¸­æå–çš„490,430é¡¹é«˜è´¨é‡RCTç»“æ„åŒ–æ•°æ®åº“ï¼Œæ”¯æŒå¤§è§„æ¨¡æ–¹æ³•å­¦å®¡è®¡ã€‚

COMPASS â€“ é¦–ä¸ªå°†å¤šæ™ºèƒ½ä½“æå–ä¸è‡ªåŠ¨åŒ–Metaåˆ†æå·¥ä½œæµé›†æˆçš„åœ¨çº¿å¹³å°ï¼Œæ”¯æŒå®æ—¶è¯æ®åˆæˆä¸æ–¹æ³•å­¦æ¢ç´¢ã€‚

ğŸš€ è®¿é—®COMPASSå¹³å°
ğŸŒ ç‚¹å‡»è®¿é—®å®æ—¶å¹³å° ğŸ‘‰ https://chatgptmodel.shinyapps.io/COMPASS/
å¹³å°æ”¯æŒï¼š

é€šè¿‡ACCORDå¤šæ™ºèƒ½ä½“å¼•æ“å®ç°è‡ªåŠ¨åŒ–æ–‡çŒ®ç­›é€‰ä¸æ•°æ®æå–

äº¤äº’å¼Metaåˆ†æï¼Œå®æ—¶ç”Ÿæˆæ£®æ—å›¾ä¸åå€šè¯Šæ–­

å¤§è§„æ¨¡æ–¹æ³•å­¦ä»ªè¡¨æ¿ï¼Œå¯è§†åŒ–49ä¸‡+RCTçš„ç»Ÿè®¡åŠŸæ•ˆã€p-hackingæ¨¡å¼åŠæ—¶åºè¶‹åŠ¿

ğŸ“Š ä¸»è¦å‘ç°
ç»Ÿè®¡åŠŸæ•ˆï¼šä»…60.4%çš„RCTå…·å¤‡è¶³å¤ŸåŠŸæ•ˆï¼ˆâ‰¥80%ï¼‰ï¼Œä½†è¿™ä¸€æ¯”ä¾‹éšæ—¶é—´é€æ­¥æ”¹å–„ã€‚

P-Hackingæ¨¡å¼ï¼šå‘ˆç°Uå‹åˆ†å¸ƒâ€”â€”åœ¨ä¸­å±‚æœŸåˆŠä»¥åŠé«˜è¢«å¼•å’Œé›¶è¢«å¼•è®ºæ–‡ä¸­æœ€ä¸ºæ™®éã€‚

æ¨¡å‹æ€§èƒ½ï¼š26ä¸ªLLMçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºæ˜¾è‘—çš„ä»»åŠ¡ä¾èµ–æ€§å·®å¼‚ï¼Œæ— å•ä¸€æ¨¡å‹åœ¨æ‰€æœ‰æå–ä»»åŠ¡ä¸­å ä¼˜ã€‚

ç³»ç»Ÿç²¾åº¦ï¼šACCORDæ¡†æ¶é€šè¿‡åè°ƒéªŒè¯ï¼Œå®ç°äº†é«˜äºä»»ä½•å•ä¸€LLMçš„å…±è¯†å‡†ç¡®ç‡ã€‚